{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto II: De palabras a Vectores\n",
    "\n",
    "En este proyecto vamos a implementar uno de los sistemas más utilizados par aconvertir palabras a vectores **word2vec**. En este proyecto la idea es que ustedes creén una base de datos de texto, la cúal utilizarán para entrenar la códigficación de estás palabras, y a partir de allí implementar ciertos sistemas. Par afacilitar el trabajo, en este notebook pueden encontrar unas funciones que fueron extraidas de esté [link](https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28), y adaptadas para trabajar con el lenguaje español. Sientanse libres de utilizar estas funciones o de adaptarlas asu gusto. Las funciones toman como entrada un texto en formato plano, y al finalizar entregan el vector de palabras de contexto y el vector target que puede ser utilizado para entrenar el modelo word2vec. Los datos de entrenamiento se entregan en una lista que contiene el vector de salida (codificación one hot de una palabra) y el vector que contiene la suma de los vectores hot encoded de las palabras de contexto. Para escoger el tamaño de la palabra de contexto deben modificar el parámetro $C$.\n",
    "\n",
    "Para este proyecto necesitan realizar lo siguiente:\n",
    "\n",
    "1. Obtengan un corpus de texto plano en Español, lo suficientemente largo, que contenga información sobre diferentes países. Este texto lo pueden extraer de wikipedia.\n",
    "2. Con la información de este corpus entrenen un modelo word2vec, aquí debe seleccionar un valor $N$ que representa la longitud del vector que representará las palabras. Este modelo es una red neuronal con una sola capa oculta de $N$ neuronas.\n",
    "3. Una vez tengan el modelo identifiquen la relación entre algunos países (no todos) de su corpus y sus capitales.\n",
    "4. Mediante esta relación encuentren las capitales de los demás países que incluyeron en el corpus. Para esto deben realizar una búsqueda de vectores cercanos a un punto en el espacio $\\mathbb{R}^{\\text{N}}$. Esta búsqueda la pueden realizar con *Locally Sensitive Hashing*.\n",
    "4. Realice una visualización en 2D o 3D de los vectores correspondientes a los países y sus capitales. Indicando los que usarón de referencia para encontrar la relación entre país y capital, y aquellos para los cuales no utilizarón esa relación.\n",
    "5. Seleccione 5 Palabras y para ellas calcule las 10 palabras más parecidas (sinonimos) en orden de similitud.\n",
    "6. Construya un nuevo corpus, más pequeño, sobre un tema cualquiera. Para este corpus necesita la versión en Español y en Ingles de los textos. Una vez más pueden utilizar Wikipedia para esto. \n",
    "7. Obtenga las representaciones en vectores par alas palabras en Español y en Ingles.\n",
    "8. De los datos obtenidos, escoga un conjunto $m$ de palabras en español, y las respectiva traducción en ingles (que se encuentren en el vocabulario que construyeron) y planteé el problema de transformacion $\\mathcal{X}\\mathbf{R} = \\mathcal{Y}$. De tal forma que encuentren la matrix de \"traducción\" $\\mathbf{R}$.\n",
    "9. Realicé esta operación para varios valores de $m$.\n",
    "10. Finalmente escoja algunas palabras en Español, apliqueles la transformación $\\mathbf{R}$ y liste las 5 palabras más cercanas en $\\mathcal{Y}$ a esta traducción.\n",
    "\n",
    "Al finalizar los diferentes puntos del proyecto, contesten las isguientes preguntas:\n",
    "\n",
    "1. ¿Qué problemas tuvierón a la hora de implementar el modelo word2vec?\n",
    "2. ¿Cómo escogierón el parámetro $N$?\n",
    "3. ¿Qué pueden decir de los resultados del modelo buscando la relación entre los países y las capitales?\n",
    "4. ¿Qué tan bien funciona el modelo para los sinonimos?, ¿Cómo creen que se puede mejorar este modelo?\n",
    "5. ¿Cómo afecta el parámetro $m$ el funcionamiento del modelo de traducción?\n",
    "6. ¿Cómo mejorarian el modelo de traducción?\n",
    "7. ¿Qué concluyen de este proyecto y de los resultados obtenidos?\n",
    "\n",
    "Al finalizar deben entregar el notebook, con los archivos de soporte (los corpus) y demás elementos que consideren necesarios. Recuerde dar respuesta a las preguntas del notebook, y comentar cada parte del proceso. El proyecto se debe entregar a más tardar el **Domingo 29 de Marzo a las 12 de la noche**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacndo librerias necesarias\n",
    "\n",
    "import re \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer y procesar el archivo en texto plano.\n",
    "\n",
    "def get_file_data(fname, stop_word_removal='no'):\n",
    "    file_contents = []\n",
    "    with open(fname) as f:\n",
    "        file_contents = f.read()\n",
    "    text = []\n",
    "    for val in file_contents.split('.'):\n",
    "        val = re.sub(r'[,¡!¿?;-]+','.',val)\n",
    "        val = re.sub(r'á','a',val)\n",
    "        val = re.sub(r'é','e',val)\n",
    "        val = re.sub(r'í','i',val)\n",
    "        val = re.sub(r'ó','o',val)\n",
    "        val = re.sub(r'ú','u',val)\n",
    "        val = re.sub(r'Á','A',val)\n",
    "        val = re.sub(r'É','E',val)\n",
    "        val = re.sub(r'Í','I',val)\n",
    "        val = re.sub(r'Ó','O',val)\n",
    "        val = re.sub(r'Ú','U',val)\n",
    "        val = re.sub(r'ñ','n',val)\n",
    "        val = re.sub(r'Ñ','N',val)\n",
    "        sent = re.findall(\"[A-Za-z]+\", val)\n",
    "        line = ''\n",
    "        for words in sent:\n",
    "            \n",
    "            if stop_word_removal == 'yes': \n",
    "                if len(words) > 1 and words not in stop_words:\n",
    "                    line = line + ' ' + words\n",
    "            else:\n",
    "                if len(words) > 1 :\n",
    "                    line = line + ' ' + words\n",
    "        text.append(line)\n",
    "    return text\n",
    "\n",
    "# Función para obtener un Vocabulario en función del texto procesado\n",
    "\n",
    "def generate_dictinoary_data(text):\n",
    "    word_to_index= dict()\n",
    "    index_to_word = dict()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    vocab_size = 0\n",
    "    \n",
    "    for row in text:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            corpus.append(word)\n",
    "            if word_to_index.get(word) == None:\n",
    "                word_to_index.update ( {word : count})\n",
    "                index_to_word.update ( {count : word })\n",
    "                count  += 1\n",
    "    vocab_size = len(word_to_index)\n",
    "    length_of_corpus = len(corpus)\n",
    "    \n",
    "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus\n",
    "\n",
    "# Función para generar representaciones one hot de los vectores target y del corpus\n",
    "\n",
    "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
    "    \n",
    "    #Create an array of size = vocab_size filled with zeros\n",
    "    trgt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    #Get the index of the target_word according to the dictionary word_to_index. \n",
    "    index_of_word_dictionary = word_to_index.get(target_word) \n",
    "    \n",
    "    #Set the index to 1\n",
    "    trgt_word_vector[index_of_word_dictionary] = 1\n",
    "    \n",
    "    #Repeat same steps for context_words but in a loop\n",
    "    ctxt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    \n",
    "    for word in context_words:\n",
    "        index_of_word_dictionary = word_to_index.get(word) \n",
    "        ctxt_word_vector[index_of_word_dictionary] = 1\n",
    "        \n",
    "    return trgt_word_vector,ctxt_word_vector\n",
    "\n",
    "# Función para generar los datos de entrenamiento para la red neuronal que representa el modelo word2vec\n",
    "\n",
    "def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,sample=None):\n",
    "\n",
    "    training_data =  []\n",
    "    training_sample_words =  []\n",
    "    for i,word in enumerate(corpus):\n",
    "\n",
    "        index_target_word = i\n",
    "        target_word = word\n",
    "        context_words = []\n",
    "\n",
    "        #when target word is the first word\n",
    "        if i == 0:  \n",
    "\n",
    "            # trgt_word_index:(0), ctxt_word_index:(1,2)\n",
    "            context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] \n",
    "\n",
    "\n",
    "        #when target word is the last word\n",
    "        elif i == len(corpus)-1:\n",
    "\n",
    "            # trgt_word_index:(9), ctxt_word_index:(8,7), length_of_corpus = 10\n",
    "            context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size  , -1 )]\n",
    "\n",
    "        #When target word is the middle word\n",
    "        else:\n",
    "\n",
    "            #Before the middle target word\n",
    "            before_target_word_index = index_target_word - 1\n",
    "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
    "                if x >=0:\n",
    "                    context_words.extend([corpus[x]])\n",
    "\n",
    "            #After the middle target word\n",
    "            after_target_word_index = index_target_word + 1\n",
    "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
    "                if x < len(corpus):\n",
    "                    context_words.extend([corpus[x]])\n",
    "\n",
    "\n",
    "        trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index)\n",
    "        training_data.append([trgt_word_vector,ctxt_word_vector])   \n",
    "        \n",
    "        if sample is not None:\n",
    "            training_sample_words.append([target_word,context_words])   \n",
    "        \n",
    "    return training_data,training_sample_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución del código\n",
    "\n",
    "fname = 'paises' #Nombre del archivo\n",
    "C = 2 # Número de palabras de contexto a la derecha y a la izquierda\n",
    "text = get_file_data(fname, stop_word_removal='no')\n",
    "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus = generate_dictinoary_data(text)\n",
    "training_data,training_sample_words = generate_training_data(corpus,C,vocab_size,word_to_index,length_of_corpus,sample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x[1] for x in training_data])\n",
    "Y = np.array([y[0] for y in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo train test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., ..., 0., 0., 0.]), array([0., 1., 1., ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  \n",
    "model.add(Dense(100,input_shape=(vocab_size,), activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='CategoricalCrossentropy', optimizer='rmsprop', metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 6.9106 - accuracy: 0.0669 - val_loss: 6.3670 - val_accuracy: 0.0849\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.9951 - accuracy: 0.0862 - val_loss: 6.3927 - val_accuracy: 0.0849\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.7676 - accuracy: 0.0862 - val_loss: 6.4421 - val_accuracy: 0.0849\n",
      "Epoch 4/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.6038 - accuracy: 0.0879 - val_loss: 6.5042 - val_accuracy: 0.0921\n",
      "Epoch 5/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.4635 - accuracy: 0.0997 - val_loss: 6.5068 - val_accuracy: 0.1012\n",
      "Epoch 6/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.3316 - accuracy: 0.1120 - val_loss: 6.5448 - val_accuracy: 0.1025\n",
      "Epoch 7/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.2030 - accuracy: 0.1218 - val_loss: 6.6062 - val_accuracy: 0.1052\n",
      "Epoch 8/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 5.0784 - accuracy: 0.1237 - val_loss: 6.5974 - val_accuracy: 0.1104\n",
      "Epoch 9/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.9522 - accuracy: 0.1291 - val_loss: 6.6915 - val_accuracy: 0.1117\n",
      "Epoch 10/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.8267 - accuracy: 0.1375 - val_loss: 6.7001 - val_accuracy: 0.1130\n",
      "Epoch 11/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.6984 - accuracy: 0.1450 - val_loss: 6.8173 - val_accuracy: 0.1104\n",
      "Epoch 12/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.5710 - accuracy: 0.1548 - val_loss: 6.8634 - val_accuracy: 0.1143\n",
      "Epoch 13/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.4409 - accuracy: 0.1621 - val_loss: 6.8659 - val_accuracy: 0.1150\n",
      "Epoch 14/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.3095 - accuracy: 0.1741 - val_loss: 6.9472 - val_accuracy: 0.1202\n",
      "Epoch 15/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.1749 - accuracy: 0.1901 - val_loss: 7.0980 - val_accuracy: 0.1215\n",
      "Epoch 16/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 4.0382 - accuracy: 0.2094 - val_loss: 7.1452 - val_accuracy: 0.1248\n",
      "Epoch 17/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.8980 - accuracy: 0.2318 - val_loss: 7.2264 - val_accuracy: 0.1248\n",
      "Epoch 18/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.7563 - accuracy: 0.2536 - val_loss: 7.2941 - val_accuracy: 0.1267\n",
      "Epoch 19/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.6096 - accuracy: 0.2794 - val_loss: 7.3715 - val_accuracy: 0.1287\n",
      "Epoch 20/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.4635 - accuracy: 0.3046 - val_loss: 7.4788 - val_accuracy: 0.1274\n",
      "Epoch 21/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.3151 - accuracy: 0.3334 - val_loss: 7.5663 - val_accuracy: 0.1280\n",
      "Epoch 22/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.1628 - accuracy: 0.3597 - val_loss: 7.7050 - val_accuracy: 0.1274\n",
      "Epoch 23/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 3.0086 - accuracy: 0.3863 - val_loss: 7.8158 - val_accuracy: 0.1248\n",
      "Epoch 24/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.8541 - accuracy: 0.4118 - val_loss: 7.9014 - val_accuracy: 0.1228\n",
      "Epoch 25/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.6954 - accuracy: 0.4314 - val_loss: 8.0451 - val_accuracy: 0.1195\n",
      "Epoch 26/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.5377 - accuracy: 0.4541 - val_loss: 8.1831 - val_accuracy: 0.1195\n",
      "Epoch 27/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.3765 - accuracy: 0.4818 - val_loss: 8.2420 - val_accuracy: 0.1208\n",
      "Epoch 28/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.2149 - accuracy: 0.5123 - val_loss: 8.3171 - val_accuracy: 0.1189\n",
      "Epoch 29/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.0504 - accuracy: 0.5434 - val_loss: 8.4232 - val_accuracy: 0.1221\n",
      "Epoch 30/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.8884 - accuracy: 0.5736 - val_loss: 8.4985 - val_accuracy: 0.1202\n",
      "Epoch 31/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.7275 - accuracy: 0.6036 - val_loss: 8.6086 - val_accuracy: 0.1189\n",
      "Epoch 32/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.5672 - accuracy: 0.6321 - val_loss: 8.7308 - val_accuracy: 0.1208\n",
      "Epoch 33/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.4097 - accuracy: 0.6685 - val_loss: 8.8319 - val_accuracy: 0.1189\n",
      "Epoch 34/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.2575 - accuracy: 0.7055 - val_loss: 8.9269 - val_accuracy: 0.1189\n",
      "Epoch 35/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.1127 - accuracy: 0.7416 - val_loss: 9.0220 - val_accuracy: 0.1195\n",
      "Epoch 36/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.9762 - accuracy: 0.7850 - val_loss: 9.0960 - val_accuracy: 0.1182\n",
      "Epoch 37/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.8497 - accuracy: 0.8208 - val_loss: 9.2252 - val_accuracy: 0.1169\n",
      "Epoch 38/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.7337 - accuracy: 0.8558 - val_loss: 9.3262 - val_accuracy: 0.1163\n",
      "Epoch 39/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.6308 - accuracy: 0.8844 - val_loss: 9.4648 - val_accuracy: 0.1137\n",
      "Epoch 40/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.5410 - accuracy: 0.9082 - val_loss: 9.5665 - val_accuracy: 0.1163\n",
      "Epoch 41/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.4634 - accuracy: 0.9244 - val_loss: 9.6642 - val_accuracy: 0.1150\n",
      "Epoch 42/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.3963 - accuracy: 0.9381 - val_loss: 9.8002 - val_accuracy: 0.1130\n",
      "Epoch 43/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.3413 - accuracy: 0.9479 - val_loss: 9.9252 - val_accuracy: 0.1150\n",
      "Epoch 44/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2944 - accuracy: 0.9541 - val_loss: 10.0325 - val_accuracy: 0.1130\n",
      "Epoch 45/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2553 - accuracy: 0.9625 - val_loss: 10.1320 - val_accuracy: 0.1130\n",
      "Epoch 46/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2234 - accuracy: 0.9664 - val_loss: 10.3020 - val_accuracy: 0.1130\n",
      "Epoch 47/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9698 - val_loss: 10.3962 - val_accuracy: 0.1104\n",
      "Epoch 48/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1738 - accuracy: 0.9740 - val_loss: 10.5362 - val_accuracy: 0.1117\n",
      "Epoch 49/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1553 - accuracy: 0.9754 - val_loss: 10.6011 - val_accuracy: 0.1104\n",
      "Epoch 50/50\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1387 - accuracy: 0.9790 - val_loss: 10.7080 - val_accuracy: 0.1091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5fc5edc6a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=50,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "peru_index      = word_to_index['peru']\n",
    "lima_index      = word_to_index['lima']\n",
    "\n",
    "nepal_index     = word_to_index['nepal']\n",
    "katmandu_index  = word_to_index['katmandu']\n",
    "\n",
    "brunei_index    = word_to_index['brunei']\n",
    "bandar_index    = word_to_index['bandar']\n",
    "\n",
    "canada_index    = word_to_index['canada']\n",
    "ottawa_index    = word_to_index['ottawa']\n",
    "\n",
    "dinamarca_index = word_to_index['dinamarca']\n",
    "copenhague_index = word_to_index['copenhague']\n",
    "\n",
    "gambia_index    = word_to_index['gambia']\n",
    "banjul_index    = word_to_index['banjul'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paises y capitales\n",
    "peru = training_data[peru_index][0]\n",
    "lima = training_data[peru_index][0]\n",
    "\n",
    "nepal = training_data[nepal_index][0]\n",
    "katmandu = training_data[katmandu_index][0]\n",
    "\n",
    "brunei = training_data[brunei_index][0]\n",
    "bandar = training_data[bandar_index][0]\n",
    "\n",
    "canada = training_data[canada_index][0]\n",
    "ottawa = training_data[ottawa_index][0]\n",
    "\n",
    "dinamarca = training_data[dinamarca_index][0]\n",
    "copenhague = training_data[copenhague_index][0]\n",
    "\n",
    "gambia = training_data[gambia_index][0]\n",
    "banjul = training_data[banjul_index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaciones entre paises y capitales\n",
    "\n",
    "r_peru   = peru-lima\n",
    "r_nepal  = nepal- katmandu\n",
    "r_brunei = brunei-bandar\n",
    "\n",
    "capital = np.array([r_peru,r_nepal,r_brunei]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(canada+capital).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input point needs to be of the same dimension as\n",
      "                  `input_dim` when initializing this LSHash instance shapes (6,8) and (1620,) not aligned: 8 (dim 1) != 1620 (dim 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,8) and (1620,) not aligned: 8 (dim 1) != 1620 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-0df829c5d21c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# query a data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lshashpy3/lshash.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, input_point, extra_data)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             table.append_val(self._hash(self.uniform_planes[i], input_point),\n\u001b[0m\u001b[1;32m    244\u001b[0m                              value)\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lshashpy3/lshash.py\u001b[0m in \u001b[0;36m_hash\u001b[0;34m(self, planes, input_point)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0minput_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_point\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for faster dot product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mprojections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplanes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             print(\"\"\"The input point needs to be an array-like object with\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,8) and (1620,) not aligned: 8 (dim 1) != 1620 (dim 0)"
     ]
    }
   ],
   "source": [
    "import lshashpy3 as lshash\n",
    "\n",
    "# create 6-bit hashes for input data of 8 dimensions:\n",
    "lsh = lshash.LSHash(6, 1620)\n",
    "\n",
    "for word in training_data:\n",
    "    lsh.index(word[0])\n",
    "\n",
    "# query a data point\n",
    "#top_n = 1\n",
    "#nn = lsh.query([1,2,3,4,5,6,7,7], num_results=top_n, distance_func=\"euclidean\")\n",
    "#print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
